# Import required Libraries
import tkinter as tk
from tkinter import *
from PIL import Image
from PIL import ImageTk
import numpy as np
import cv2
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

# Build the convolution network architecture
face_model = Sequential()
face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))
face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
face_model.add(MaxPooling2D(pool_size=(2, 2)))
face_model.add(Dropout(0.25))
face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
face_model.add(MaxPooling2D(pool_size=(2, 2)))
face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
face_model.add(MaxPooling2D(pool_size=(2, 2)))
face_model.add(Dropout(0.25))
face_model.add(Flatten())
face_model.add(Dense(1024, activation='relu'))
face_model.add(Dropout(0.5))
face_model.add(Dense(7, activation='softmax'))

# Load the saved weights
face_model.load_weights('recognition_model.h5')

# Disable OpenCL
cv2.ocl.setUseOpenCL(False)

# Create Datasets Dictionaries
facial_dict = {
0: "   Angry   ", 
1: "Disgusted", 
2: "  Fearful  ",
3: "   Happy   ",
4: "  Neutral  ", 
5: "    Sad    ", 
6: "Surprised"}

emojis_dict = {
0:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/angry.png",
1:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/disgusted.png",
2:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/fearful.png",
3:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/happy.png",
4:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/neutral.png",
5:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/sad.png",
6:"/Users/saimoulikabedadhala/Desktop/Internship /Emojis/surprised.png"}

# Global variables
global last_frame1
last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)
global cap1
show_text=[0]

# Function to get face captured and recognize emotion
def Capture_Image():
    global cap1
    #video_capture = cv2.VideoCapture(0) Here we are accessing your camera. 
    # When the parameter is 0, we are accessing an internal camera from your computer. 
    # When the parameter is 1, we are accessing an external camera that is plugged on your computer.
    cap1 = cv2.VideoCapture(0)
    if not cap1.isOpened():
        print("Cant open the Camera")   
    # Capture the video frame by frame
    #.read() in OpenCV returns 2 things, boolean and data.
    flag1, frame1 = cap1.read()
    # follows the order (width, height)
    frame1 = cv2.resize(frame1,(600,500))
    # It will detect the face in the video and bound it with a rectangular box
    bound_box = cv2.CascadeClassifier('/Users/saimoulikabedadhala/Desktop/Internship /haarcascade_frontalface_default.xml')
    # converts RGB into grayscale 
    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)

    n_faces = bound_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)

    # We will loop through each rectangle (each face detected) using its coordinates
    # generated by the function detect Multi scale

    for (x, y, w, h) in n_faces:
        #cv2.rectangle(image, start_point, end_point, color, thickness)
        #cv2.rectangle() method is used to draw a rectangle on any image (blue colour)
        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)

        roi_frame = gray_frame[y:y + h, x:x + w]
        #numpy.expand_dims(a, axis):Expand the shape of an array.
        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)
        #Predicts the img using the trained model
        prediction = face_model.predict(crop_img)
        #np.argmax:Returns the indices of the maximum values along an axis
        maxindex = int(np.argmax(prediction))
        #cv2.putText() method is used to draw a text string on any image
        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
        show_text[0]=maxindex

    if flag1 is None:
        print ("Error!")

    elif flag1:
        global last_frame1
        last_frame1 = frame1.copy()
        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image
        img = Image.fromarray(pic)
        imgtk = ImageTk.PhotoImage(image=img)
        lmain.imgtk = imgtk
        lmain.configure(image=imgtk)
        lmain.after(10, Capture_Image)

    #waitkey() function of Python OpenCV allows users to display a window for given milliseconds 
    # or until any key is pressed. It takes time in milliseconds as a parameter and waits for the given time 
    # to destroy the window, if 0 is passed in the argument it waits till any key is pressed. 
    if cv2.waitKey(1) & 0xFF == ord('q'):
        exit()

# Function for showing Emoji According to Facial Expression
def Get_Emoji():
    frame2=cv2.imread(emojis_dict[show_text[0]])
    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)
    img2=Image.fromarray(frame2)
    imgtk2=ImageTk.PhotoImage(image=img2)
    lmain2.imgtk2=imgtk2
    lmain3.configure(text=facial_dict[show_text[0]],font=('arial',45,'bold'))
    lmain2.configure(image=imgtk2)
    lmain2.after(10, Get_Emoji)

# GUI Window to show captured image with emoji
if __name__ == '__main__':
    root=tk.Tk()
    heading = Label(root,bg='black')
    heading.pack()
    #The padx and the pady parameters put some space between the widgets.
    #The padx puts some space between the button widgets and between the closeButton and the right border of the root window. 
    #The pady puts some space between the button widgets and the borders of the frame and the borders of the root window.
    heading2=Label(root,text="Emojify",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output
    heading2.pack()
    #Tkinter Label is a widget that is used to implement display boxes where you can place text or images.
    lmain = tk.Label(master=root,padx=50,bd=10)
    lmain2 = tk.Label(master=root,bd=10)
    lmain3=tk.Label(master=root,bd=10,fg="#CDCDCD",bg='black')
    #the pack() method declares the position of widgets in relation to each other.
    lmain.pack(side=LEFT)
    lmain.place(x=50,y=250)
    lmain3.pack()
    lmain3.place(x=960,y=250)
    lmain2.pack(side=RIGHT)
    lmain2.place(x=900,y=350)
    root.title("Emojify")
    root.geometry("1400x900+100+10")
    root['bg']='black'
    #destroy:destroy the GUI components to free the memory as well as clear the screen
    exitbutton = Button(root, text='Quit',fg="red",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)
    #b=Button(root,text="QUIT",command=root.destroy).pack(side=BOTTOM)
    Capture_Image()
    Get_Emoji()
    root.mainloop()
    #mainloop() is simply a method in the main window that executes what we wish to execute in an application
    #it will loop forever until the user exits the window or waits for any events from the user.